# Data quality check
# Generated by FlowPilot Template Generator

# Basic Information
name: "dq_check"
title: "Data quality check"
description: "This workflow checks the data quality issues in splunk"
category: "data_quality"
complexity: "beginner"
version: "1.0.0"
author: "community"

# Requirements
splunk_versions: [8.0+, 9.0+]
required_permissions: [search, _internal index, all indexes]

# Business Context
business_value: "helps detect data quality issue during data onboarding process"
use_cases:
  - "detect data quality issues"
  - "Improve search and overall system performance"
success_metrics:
  - "no line breaking, time stamp parsing or truncation errors"
  - "performant indexing pipelines queues"
target_audience:
  - "admin"

# Simple workflow - direct searches
searches:
  - name: "dq_ingestion_latency"
    title: "Ingestion Latency by Sourcetype"
    description: "Median and p95 of index-time minus event time (seconds); large gaps suggest clock skew or delayed delivery."
    spl: |
      index=* _time=* 
      | eval latency=_indextime - _time
      | where latency>=0 AND latency<86400
      | stats median(latency) as p50, perc95(latency) as p95 by sourcetype
      | sort - p95
    earliest: "-24h@h"
    latest: "now"
    expected_results: "Per-sourcetype p50/p95 latency (s); investigate high p95 values"
    notes: "Use _index_earliest/_index_latest if you want to bound by indextime window."
    references:
      - "Time modifiers for _indextime (_index_earliest/_index_latest).  [oai_citation:0‡help.splunk.com](https://help.splunk.com/en/splunk-cloud-platform/spl-search-reference/9.2.2406/time-format-variables-and-modifiers/time-modifiers?utm_source=chatgpt.com)"
      - "Troubleshooting event indexing delay (root causes & guidance).  [oai_citation:1‡docs.splunk.com](https://docs.splunk.com/Documentation/Splunk/9.4.2/Troubleshooting/Troubleshootingeventsindexingdelay?utm_source=chatgpt.com)"

  - name: "dq_timestamp_parse_warnings"
    title: "Timestamp Parse Warnings"
    description: "Scans splunkd logs for DateParser/strptime warnings that indicate TIME_FORMAT/TIME_PREFIX issues."
    spl: |
      index=_internal source=*splunkd.log* 
      ("DateParserVerbose" OR "could not use strptime" OR "Failed to parse timestamp")
      | rex field=_raw max_match=1 "(?i)sourcetype=(?<sourcetype>[^ ]+)"
      | timechart span=15m count by sourcetype limit=15
    earliest: "-24h@h"
    latest: "now"
    expected_results: "Time series of parse warnings by sourcetype; spikes = broken timestamping"
    references:
      - "Resolve data quality issues: incorrect timestamp extraction.  [oai_citation:2‡docs.splunk.com](https://docs.splunk.com/Documentation/Splunk/9.4.2/Data/Resolvedataqualityissues?utm_source=chatgpt.com)"
      - "Community threads showing 'could not use strptime' patterns.  [oai_citation:3‡Splunk Community](https://community.splunk.com/t5/Getting-Data-In/could-not-use-the-strptime-to-parse-timestamp-from-quot-xx-xx-xx/m-p/416926?utm_source=chatgpt.com)"

  - name: "dq_line_breaking_risk"
    title: "Very Large Events (Line Breaking Risk)"
    description: "Flags unusually large events that could indicate bad line breaking or TRUNCATE behavior."
    spl: |
      index=* 
      | eval ev_len=len(_raw)
      | where ev_len >= 10000
      | stats count as large_events, max(ev_len) as max_event_len by sourcetype
      | sort - large_events
    earliest: "-24h@h"
    latest: "now"
    expected_results: "Per-sourcetype counts of very large events; review props.conf (LINE_BREAKER/SHOULD_LINEMERGE/TRUNCATE)"
    references:
      - "Configure event line breaking.(https://docs.splunk.com/Documentation/Splunk/9.4.2/Data/Configureeventlinebreaking?utm_source=chatgpt.com)"
      - "Resolve data quality issues: line breaking & truncation symptoms.(https://docs.splunk.com/Documentation/Splunk/9.4.2/Data/Resolvedataqualityissues?utm_source=chatgpt.com)"
      - "TRUNCATE behavior context (community).(https://community.splunk.com/t5/Getting-Data-In/Why-are-larger-events-are-truncated-10000-bytes/m-p/122885?utm_source=chatgpt.com)"

  - name: "dq_duplicate_events_hash"
    title: "Duplicate Events (Hash-Based)"
    description: "Detects potential duplicates by hashing _raw per sourcetype/source in window; tune window to avoid false positives."
    spl: |
      index=* 
      | eval sig=md5(_raw)
      | stats count as cnt by sourcetype, source, sig
      | where cnt>1
      | stats sum(cnt) as duplicate_events, dc(sig) as duplicate_signatures by sourcetype, source
      | sort - duplicate_events
    earliest: "-6h@h"
    latest: "now"
    expected_results: "Duplicate counts/signatures by sourcetype/source"
    references:
      - "Community patterns for finding duplicates.(https://community.splunk.com/t5/Splunk-Search/How-do-I-find-all-duplicate-events/m-p/9764?utm_source=chatgpt.com)"

  - name: "dq_missing_time_or_future_past"
    title: "Events with Problematic _time"
    description: "Finds events with _time in the future/past beyond a threshold, a common sign of bad timestamp parsing or TZ."
    spl: |
      index=* 
      | eval now=now(), skew=_time-now
      | eval time_issue=case(skew>3600,"future_gt_1h", skew<-31536000,"past_gt_1y", true(),null())
      | where isnotnull(time_issue)
      | stats count by time_issue, sourcetype
      | sort - count
    earliest: "-24h@h"
    latest: "now"
    expected_results: "Counts of future/past-skewed events by sourcetype"
    references:
      - "Resolve data quality issues: incorrect timestamp extraction.(https://docs.splunk.com/Documentation/Splunk/9.4.2/Data/Resolvedataqualityissues?utm_source=chatgpt.com)"

  - name: "dq_field_coverage_summary"
    title: "Field Coverage Summary"
    description: "Summarizes field presence/null rates to spot broken extractions after ingestion changes."
    spl: |
      index=* 
      | head 5000
      | fieldsummary
      | eval null_pct=round(100*(isnull)/count,2)
      | table field, distinct_values, null_pct, maxlen, avglen
      | sort - null_pct
    earliest: "-2h@h"
    latest: "now"
    expected_results: "At-a-glance field health (null %, distinctness, len); high null_pct on key fields = extraction issues"
    references:
      - "props.conf field extraction context (index/search time).(https://docs.splunk.com/Documentation/Splunk/9.4.2/Admin/Propsconf?utm_source=chatgpt.com)"

  - name: "dq_source_host_coverage"
    title: "Source & Host Coverage (Active Last 24h)"
    description: "Distinct hosts and sources per sourcetype; useful to catch missing senders or misrouted data."
    spl: |
      index=* 
      | stats dc(host) as hosts, dc(source) as sources by sourcetype
      | sort - hosts
    earliest: "-24h@h"
    latest: "now"
    expected_results: "Per-sourcetype counts of active hosts and sources"

  - name: "dq_event_rate_stability"
    title: "Event Rate Stability (Hour over Hour)"
    description: "Compares last hour vs prior day same hour average to flag big drops/spikes in volume."
    spl: |
      | tstats count WHERE index=* by _time, sourcetype span=1h
      | eval hour=strftime(_time,"%H")
      | eventstats avg(count) as prior_avg by sourcetype, hour
      | eval delta=count-prior_avg, pct=round(100*delta/prior_avg,2)
      | where abs(pct)>=50 AND prior_avg>0
      | table _time, sourcetype, count, prior_avg, pct
      | sort - _time
    earliest: "-25h@h"
    latest: "now"
    expected_results: "Sourcetypes with >=50% change vs typical same-hour baseline"

  - name: "dq_nullqueue_suspect_queue_pressure"
    title: "Parsing Queue Pressure (Drop Risk Proxy)"
    description: "Queue fill % as a proxy for parsing pressure that can lead to data loss or delays."
    spl: |
      index=_internal source=*metrics.log group=queue
      | stats avg(current_size_kb) as size_kb, avg(max_size_kb) as max_kb by name
      | eval fill_pct=round(100*size_kb/max_kb,2)
      | sort - fill_pct
    earliest: "-1h@h"
    latest: "now"
    expected_results: "Queue fill levels; sustained high % indicates bottlenecks"
    references:
      - "Resolve data quality issues (pipeline/queue backpressure context).(https://docs.splunk.com/Documentation/Splunk/9.4.2/Data/Resolvedataqualityissues?utm_source=chatgpt.com)"

  - name: "dq_index_storage_headroom"
    title: "Index Storage Headroom"
    description: "Checks total index size vs configured max to spot near-capacity indexes (can cause data aging/truncation)."
    spl: |
      | rest /services/data/indexes
      | eval current_size_gb=round(currentDBSizeMB/1024,2)
      | eval max_gb=case(isnotnull(maxTotalDataSizeMB), round(maxTotalDataSizeMB/1024,2), true(), null())
      | eval pct_of_max=if(isnotnull(max_gb) AND max_gb>0, round(100*current_size_gb/max_gb,2), null())
      | table title, totalEventCount, current_size_gb, max_gb, pct_of_max, frozenTimePeriodInSecs
      | sort - pct_of_max
    expected_results: "Per-index size and % of configured max; investigate high pct_of_max"
    references:
      - "Resolve data quality issues (aging/storage), and REST indexes fields.(https://docs.splunk.com/Documentation/Splunk/9.4.2/Data/Resolvedataqualityissues?utm_source=chatgpt.com)"

  - name: "dq_line_breaking_candidates_xml_json"
    title: "Line-Breaking Candidates (XML/JSON)"
    description: "Heuristic: XML/JSON sources with very large lines often need explicit LINE_BREAKER/SHOULD_LINEMERGE configs."
    spl: |
      index=* (sourcetype=*xml* OR sourcetype=*json* OR source="*.xml" OR source="*.json")
      | eval ev_len=len(_raw)
      | where ev_len>=10000
      | stats count as large_events, max(ev_len) as max_event_len by sourcetype, source
      | sort - large_events
    earliest: "-24h@h"
    latest: "now"
    expected_results: "XML/JSON sources most likely to suffer from bad line breaking"
    references:
      - "Configure event line breaking. (https://docs.splunk.com/Documentation/Splunk/9.4.2/Data/Configureeventlinebreaking?utm_source=chatgpt.com)"
      - "Resolve data quality issues (line breaking).(https://docs.splunk.com/Documentation/Splunk/9.4.2/Data/Resolvedataqualityissues?utm_source=chatgpt.com)"